{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "String Dataset Pre-processing Methods.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunadacosta/Master-Thesis/blob/master/String_Dataset_Pre_processing_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xNXbuRDc4lw",
        "colab_type": "text"
      },
      "source": [
        "# String Dataset Pre-processing Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XSX4F8FdAFg",
        "colab_type": "text"
      },
      "source": [
        "Created by: Luciana G. da Costa\n",
        "\n",
        "A study on pre-processing and vectorization methods of string inputs for the traing of NNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOzn2wKFdO5x",
        "colab_type": "text"
      },
      "source": [
        "Mount Google drive and Load dataset with Pandas. Also spliting the dataset. This part was a test for future use, does ot have big relevancy to this study."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mq0JpOpdfUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "6390aa6c-67b5-498b-f8ce-97abf0233979"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEH3ZDJZdgZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "66733df8-8eeb-4ee9-b0b2-dfc1ea2d0480"
      },
      "source": [
        "import pandas as pd\n",
        "##### Load data with Pandas ######\n",
        "\n",
        "filepath_dict = {'spam': 'gdrive/My Drive/Colab Notebooks/dataset/spam.csv',\n",
        "                 'ham': 'gdrive/My Drive/Colab Notebooks/dataset/ham.csv'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['symbols', 'label'], sep=';')\n",
        "    df['source'] = source  #Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0:10,:])\n",
        "### randomize dataset\n",
        "#frame = df.sample(frac=1).reset_index(drop=True)\n",
        "#print(frame.iloc[0:10,:])\n",
        "\n",
        "############## split the dataset in train and test dataset ##############\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    sentences = df_source['symbols'].values\n",
        "    y = df_source['label'].values\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=42)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             symbols  label source\n",
            "0               ARC_NA              DATE_IN_PAST ...      1   spam\n",
            "1               ARC_NA              DATE_IN_PAST ...      1   spam\n",
            "2               ARC_NA              FROM_HAS_DN  ...      1   spam\n",
            "3               ARC_NA              FROM_NEQ_DISP...      1   spam\n",
            "4               ARC_NA              R_SPF_DNSFAIL...      1   spam\n",
            "5               ARC_NA              SUBJ_EXCESS_B...      1   spam\n",
            "6               ARC_NA              R_SPF_DNSFAIL...      1   spam\n",
            "7               ARC_NA              SUBJECT_ENDS_...      1   spam\n",
            "8               ARC_NA              FROM_HAS_DN  ...      1   spam\n",
            "9               ARC_NA              DATE_IN_PAST ...      1   spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yruy3dNBd8Bd",
        "colab_type": "text"
      },
      "source": [
        "# Word Counts with CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWI8S3N2eBDk",
        "colab_type": "text"
      },
      "source": [
        "The vectorizer object provided by Scikit-Learn [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is a very simple way to both tokenize a collection of text documents and build a vocabulary of know words, but also encode new documents using that vocabulary.\n",
        "\n",
        "Itt returns a sparse vector with the length of the entire vocabulary and a n interger count for the number of times each word appeared in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYC0EOntgAjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "035dec43-f337-4ae6-9ddf-8495a26940f6"
      },
      "source": [
        "##### Tokenize using CountVectorizer#######\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# create the transform  \n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize an create the vocabulary\n",
        "vectorizer.fit(sentences_train)\n",
        "#summarize\n",
        "vocab = vectorizer.vocabulary_\n",
        "print(\"Vocabulary: \",vocab)\n",
        "# encode document\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test = vectorizer.transform(sentences_test)\n",
        "# summarize encoded vector\n",
        "print(\"Train vocab shape: \", X_train.shape)\n",
        "print(\"Train type: \",type(X_train))\n",
        "print(\"Train vector array: \",X_train.toarray())\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary:  {'arc_na': 0, 'mv_case': 14, 'mime_good': 11, 'to_dn_none': 30, 'dmarc_dnsfail': 2, 'from_no_dn': 7, 'rcpt_count_twelve': 21, 'rcvd_count_zero': 23, 'mime_trace': 12, 'r_dkim_na': 15, 'date_in_past': 1, 'mid_rhs_not_fqdn': 10, 'hfilter_hostname_unknown': 8, 'rcpt_count_one': 18, 'to_dom_eq_from_dom': 32, 'rcpt_count_three': 20, 'fake_reply': 4, 'subj_all_caps': 24, 'rcpt_count_seven': 19, 'rcpt_count_five': 16, 'rcpt_count_two': 22, 'rcpt_count_gt_50': 17, 'to_dn_some': 31, 'suspicious_recips': 29, 'subject_ends_exclaim': 26, 'missing_to': 13, 'introduction': 9, 'empty_subject': 3, 'subject_ends_question': 27, 'freemail_from': 6, 'freemail_envfrom': 5, 'subj_excess_qp': 25, 'subject_has_question': 28, 'to_eq_from': 33}\n",
            "Train vocab shape:  (752, 34)\n",
            "Train type:  <class 'scipy.sparse.csr.csr_matrix'>\n",
            "Train vector array:  [[1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " ...\n",
            " [1 1 1 ... 0 1 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L1eInfxivtn",
        "colab_type": "text"
      },
      "source": [
        "# Hashing with HashingVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtgeqtTRi19u",
        "colab_type": "text"
      },
      "source": [
        "Counting the frequency can be very helpful, but the problem is that the vocabulary can become very large.\n",
        "\n",
        "A clever work-around is to use one way hash of strings to convert them to intergers. The clever part of it is that no vocabulary is needed and one can choose an arbitrary-long fixed length vector. The downside of this method is that there is no way to do inverted encoding (decoding).\n",
        "\n",
        "The [HashingVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) class can be used to implement this approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF_0jEerjsAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "d8c8dbe4-e041-4b3b-efd0-72936063176a"
      },
      "source": [
        "#### Tokenize using Hashing_trick #######\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "#estimate the size of the vocabulary\n",
        "vocab_size = len(vectorizer.vocabulary_)\n",
        "print(\"Vocab size: \",vocab_size)\n",
        "#integer encode the document\n",
        "vectorizer = HashingVectorizer(n_features=vocab_size*2)\n",
        "#encode document\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test = vectorizer.transform(sentences_test)\n",
        "\n",
        "print(\"Train Vocab Shape: \",X_train.shape)\n",
        "print(\"Train Vector array: \",X_train.toarray())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  34\n",
            "Train Vocab Shape:  (752, 68)\n",
            "Train Vector array:  [[0.33333333 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.33333333 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.33333333 0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.31622777 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.31622777 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.31622777 0.         0.         ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK9mgUeRkojA",
        "colab_type": "text"
      },
      "source": [
        "# One-Hot-Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nOBZm_Sk5gR",
        "colab_type": "text"
      },
      "source": [
        "A [one-hot-encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to interger values. Then, each interger value is represented as a binary vector that is all zero values except the index of the interger, which is marked with a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4bNYKz7lb4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "8da387f2-6e2a-48bc-c3fe-396bfbd523e8"
      },
      "source": [
        "#### one-hot encoding with scikit-learn ######\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import argmax\n",
        "\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(sentences_train)\n",
        "print(integer_encoded)\n",
        "# binary encoded\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "# invertz first 10 inputs\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0:10,:])])\n",
        "print(inverted)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[31 31 26 41 31 41 41 47 12 29 23 29 38 26 19 41 41 41 35 26 23 41 38 31\n",
            " 31 47 41 23 41 47 23 38 27 41 38 41 41 47 47 47 51 19 41 38 41 41  8 41\n",
            " 16 29 39 51 38 29 26 10 10 31  9  7 47 28 29 31 47  9 23 31 38 47 31 43\n",
            " 31 23 38 23 43 41 35 47 41 34 29 26 23 26 29 23  7  9 16 47 41 31 26 27\n",
            " 26 23 47 31  7 41 47 10  6 28 23 29 41 23 38 16 19 31  5 41 26 23  7 31\n",
            " 29 29 16  2 29  7 23 41 47 31 41 27 23 23  7 47 31 16 23 41 15  9 38 47\n",
            " 31 27 49 32 41 23 41 27 29 15 29  0 26  9  7  1 31 15 47 23 31 16 15 31\n",
            " 41 41 16 38 31 29  7  9 31 48 31 41 27 23  7 41 41 26 41 47 16 32 41 41\n",
            " 28 32  9 47 41 14 41 31 26 27 31 41 26 14 41 26 31 19 23 31 41 31 37 47\n",
            " 31  7 16 19 41 41 23 47 41 31 41 29 15 41 41  7 14 11 41 47 27 41 23 41\n",
            " 24 41 41 41 41 31 16 29 23 19 23 26 19 31 41 14 31 32 15 26 47 41 38  7\n",
            "  2 15 16 27  7  7 31 26 31 41 41 29 41 26 15 47 26 38 41 31 41 23 47 23\n",
            " 41 29 23 38 46 41 31 16 29 41 47 26 47 23 27 29 31 31 41 41  3 19 29 16\n",
            " 31 47 31 16 27 31 47 26  7 23 26 26 41 31  8  7 23 41 41 41 29 41 29 41\n",
            " 15 23 41 41 41 19 10 50 41 31 31 19 26 31 31 31 41 23 41 23 38 48 27 26\n",
            " 41 26 16 31 16 41 40 38  0 26 41 41 31 41 23 26 29 31 41 31 32 23 28 26\n",
            " 29 41 14 47 15  9 47 47 38 47 29 47 29 31 41 31 28 47 41 44 32 41 31 41\n",
            " 41 31 16 47  7 41 26 26 41 29 38 30 41 38 16 20 29 38 47 28 31 26 41 13\n",
            " 41 31 41 34 26 26 41 29 38  7 41 23 41 41 29 26 14 19 31 27  7 31 41 31\n",
            " 27 36 31 23 19 38  7 41 31 26 41  7 31 29 38  7 41  9 29 31 15 16  7 23\n",
            " 41 31 23 26 34 31 26 14 15 41 16 41 38 41 41 47 43 47 31 26  5 37 31 41\n",
            " 41 23 38 47 29 26 41 16 16 47 16 28 41 29 31 39 17 31 41 29 47 41 23  9\n",
            " 29 27 32 31 41 41 22 41 41 41 25 16 27 19 26 41 23 31 16 31 47 31 41 47\n",
            " 31 19 31 23 29 41  4  7 38 26 41 47 31 29 15 41 28 26 19 41 41 29 26 23\n",
            " 41 41  9 31 33 18 31 41 41 14 29 29 44 47 26 42 47 47 29 27 23 23 29 47\n",
            " 30 43 47 47 41 47 47 41 41 47 29 47 41  1 47 38 41 31 16 41 41 47 47 41\n",
            " 15 16 31 41 30 29 31 43 29 29 38 41 31 38 27 38 23 27 37 29 38  4  7 29\n",
            "  4 41 16 26 23 47 28 43 23 16 41 27 29 38 43 31 31 23 23 47 31 31 31 47\n",
            " 26 31 31 29 41 41 23 26 23 41 23 41 41 21 41 41 47 47  7 38 41 38 23 19\n",
            " 41 26 16 29 40 41 23 29 41 27 31 31 43 29  9  1 31 19 27 23 29 14 41 41\n",
            " 41 41 38 31  9 31 45 26 37 23 23 43  1 31 43 41 26 15 16 23 29  7 29  1\n",
            " 29 29  7 32 47 41 15 41]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "['             ARC_NA              MV_CASE              MIME_GOOD              TO_DN_NONE              DMARC_DNSFAIL              FROM_NO_DN              RCPT_COUNT_TWELVE              RCVD_COUNT_ZERO              MIME_TRACE              R_DKIM_NA              DATE_IN_PAST              MID_RHS_NOT_FQDN              HFILTER_HOSTNAME_UNKNOWN ']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_WR3NSMmKPH",
        "colab_type": "text"
      },
      "source": [
        "# Keras Tokenizer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHuJvxW4mODF",
        "colab_type": "text"
      },
      "source": [
        "Keras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects.\n",
        "\n",
        "Keras provides the [Tokenizer class](https://keras.io/preprocessing/text/#tokenizer) for preparing text documents for deep learning. The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MUVVRg8mvTH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "61c637f0-0878-4f55-8320-8160a9ad4e8e"
      },
      "source": [
        "##### Tokenize using Tokenizer API #######\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer(\n",
        "    char_level=False,\n",
        "    filters=\"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n\", # will prevent ' _ ' of being removed\n",
        "    lower=False\n",
        ")\n",
        "# fit the tokenizer on the docs\n",
        "t.fit_on_texts(sentences_train)\n",
        "\n",
        "# Vocab of words and their counts\n",
        "print(\"Count of words: \",t.word_counts)\n",
        "# Vocab of words and how many documents each appeared in\n",
        "print(\"How many times each words appears in docs: \",t.document_count)\n",
        "#Vocab of words and their uniquely assigned intergers\n",
        "print(\"Assigned intergers: \",t.word_index)\n",
        "# An interger count of the total number of documents that were used to fit the Tokenizer\n",
        "print(\"Total nr. of docs use to tokenize: \",t.word_docs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of words:  OrderedDict([('ARC_NA', 752), ('MV_CASE', 752), ('MIME_GOOD', 752), ('TO_DN_NONE', 744), ('DMARC_DNSFAIL', 752), ('FROM_NO_DN', 752), ('RCPT_COUNT_TWELVE', 131), ('RCVD_COUNT_ZERO', 752), ('MIME_TRACE', 752), ('R_DKIM_NA', 752), ('DATE_IN_PAST', 752), ('MID_RHS_NOT_FQDN', 752), ('HFILTER_HOSTNAME_UNKNOWN', 752), ('RCPT_COUNT_ONE', 261), ('TO_DOM_EQ_FROM_DOM', 209), ('RCPT_COUNT_THREE', 87), ('FAKE_REPLY', 131), ('SUBJ_ALL_CAPS', 17), ('RCPT_COUNT_SEVEN', 99), ('RCPT_COUNT_FIVE', 82), ('RCPT_COUNT_TWO', 52), ('RCPT_COUNT_GT_50', 34), ('TO_DN_SOME', 2), ('SUSPICIOUS_RECIPS', 28), ('SUBJECT_ENDS_EXCLAIM', 11), ('MISSING_TO', 13), ('INTRODUCTION', 2), ('EMPTY_SUBJECT', 7), ('SUBJECT_ENDS_QUESTION', 6), ('FREEMAIL_FROM', 2), ('FREEMAIL_ENVFROM', 2), ('SUBJ_EXCESS_QP', 1), ('SUBJECT_HAS_QUESTION', 1), ('TO_EQ_FROM', 1)])\n",
            "How many times each words appears in docs:  752\n",
            "Assigned intergers:  {'ARC_NA': 1, 'MV_CASE': 2, 'MIME_GOOD': 3, 'DMARC_DNSFAIL': 4, 'FROM_NO_DN': 5, 'RCVD_COUNT_ZERO': 6, 'MIME_TRACE': 7, 'R_DKIM_NA': 8, 'DATE_IN_PAST': 9, 'MID_RHS_NOT_FQDN': 10, 'HFILTER_HOSTNAME_UNKNOWN': 11, 'TO_DN_NONE': 12, 'RCPT_COUNT_ONE': 13, 'TO_DOM_EQ_FROM_DOM': 14, 'RCPT_COUNT_TWELVE': 15, 'FAKE_REPLY': 16, 'RCPT_COUNT_SEVEN': 17, 'RCPT_COUNT_THREE': 18, 'RCPT_COUNT_FIVE': 19, 'RCPT_COUNT_TWO': 20, 'RCPT_COUNT_GT_50': 21, 'SUSPICIOUS_RECIPS': 22, 'SUBJ_ALL_CAPS': 23, 'MISSING_TO': 24, 'SUBJECT_ENDS_EXCLAIM': 25, 'EMPTY_SUBJECT': 26, 'SUBJECT_ENDS_QUESTION': 27, 'TO_DN_SOME': 28, 'INTRODUCTION': 29, 'FREEMAIL_FROM': 30, 'FREEMAIL_ENVFROM': 31, 'SUBJ_EXCESS_QP': 32, 'SUBJECT_HAS_QUESTION': 33, 'TO_EQ_FROM': 34}\n",
            "Total nr. of docs use to tokenize:  defaultdict(<class 'int'>, {'ARC_NA': 752, 'FROM_NO_DN': 752, 'RCVD_COUNT_ZERO': 752, 'DATE_IN_PAST': 752, 'HFILTER_HOSTNAME_UNKNOWN': 752, 'MV_CASE': 752, 'DMARC_DNSFAIL': 752, 'MIME_GOOD': 752, 'MID_RHS_NOT_FQDN': 752, 'RCPT_COUNT_TWELVE': 131, 'R_DKIM_NA': 752, 'MIME_TRACE': 752, 'TO_DN_NONE': 744, 'RCPT_COUNT_ONE': 261, 'TO_DOM_EQ_FROM_DOM': 209, 'RCPT_COUNT_THREE': 87, 'FAKE_REPLY': 131, 'SUBJ_ALL_CAPS': 17, 'RCPT_COUNT_SEVEN': 99, 'RCPT_COUNT_FIVE': 82, 'RCPT_COUNT_TWO': 52, 'RCPT_COUNT_GT_50': 34, 'TO_DN_SOME': 2, 'SUSPICIOUS_RECIPS': 28, 'SUBJECT_ENDS_EXCLAIM': 11, 'MISSING_TO': 13, 'INTRODUCTION': 2, 'EMPTY_SUBJECT': 7, 'SUBJECT_ENDS_QUESTION': 6, 'FREEMAIL_FROM': 2, 'FREEMAIL_ENVFROM': 2, 'SUBJ_EXCESS_QP': 1, 'SUBJECT_HAS_QUESTION': 1, 'TO_EQ_FROM': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
